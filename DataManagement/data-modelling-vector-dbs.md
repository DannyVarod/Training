# Data modelling and vector databases

When working with vector databases, data modelling focuses on how to structure and organize your documents, embeddings, and metadata to optimize for similarity search, retrieval performance, and specific AI/ML use cases.

Unlike traditional databases that optimize for exact matches and structured queries, vector databases are designed around semantic similarity and approximate nearest neighbor (ANN) search.

## Key Concepts

### Embeddings

**Embeddings** are dense vector representations of your data (text, images, audio, etc.) generated by machine learning models. The quality and dimensionality of embeddings directly impact search accuracy and performance. Try to find a balance between embedding size and search speed, for instance `all-MiniLM-L6-v2` and `all-MiniLM-L12-v2` (384 dimensions) are good general-purpose model for text with a good trade-off between quality and performance.

See [SBERT.net's sentence transformer pretrained models](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html) for details on common models available in hugging-face.

### Metadata

**Metadata** are traditional attributes that can be used for filtering and organizing your vectors. This enables hybrid search combining semantic similarity with exact attribute matching.
Use metadata to store information like the source of the document, creation date, category, tags, and any other relevant attributes that can help refine search results.

### Collections

**Collections** are logical groupings of vectors, similar to tables in relational databases or collections in document databases. Each collection typically contains vectors of the same type and dimensionality, for example, a collection with description of products and another collection with technical support documentation.

## Data Modelling Strategies

### Document-Centric Approach

Store entire documents as single vectors with comprehensive metadata.

**When to use**:

- Short documents (<1000 words)

### Chunk-Based Approach

Split large documents into smaller, semantically meaningful chunks.

**When to use**:

- Large documents (>1000 words)
- Question-answering systems
- When context window limitations exist (which is usually the case)

#### Markdown-based Chunking

A common approach for chunking text documents is to use markdown structure to define chunks.
For example, use headings (`#`, `##`, `###`) to define sections and sub-sections, and split content accordingly.

The benefits of this approach are:

- Markdown is widely used for documentation
- Splitting using headings preserves structure of original documents
- Markdown is readable and easy to edit
- Markdown parsers are available in many programming languages
- Many formats (HTML, PDF, etc.) can be converted to markdown
- LLMs can understand markdown formatting and structure and even generate markdown content themselves

### Entity-Centric Approach

Create vectors for specific entities (products, users, etc.) with rich attribute data.

**When to use**:

- When you have well-defined entities with non-structured information about them and you want to find the most suitable entities based on a non-structured query

## Embedding Strategies

### Single Embedding Per Document

Generate one vector representation for the entire content.

**Advantages**:

- Simple to implement and manage
- Good for document-level similarity
- Lower storage requirements

**Disadvantages**:

- May lose specific details in large documents
- Less precise for targeted information retrieval

### Multiple Embeddings Per Document

Generate separate embeddings for different aspects (title, content, summary).

**When to use**:

- Multi-modal search requirements
- Different similarity criteria for different fields
- Advanced ranking strategies

### Hierarchical Embeddings

Create embeddings at different levels of granularity.
E.g. heading-1-level, heading-2-level, heading-3-level, paragraph-level, etc.

### Categorical Embeddings

Given a set of categories or tags, you can classify the query into categories and use the result to find relevant documents.

#### Fixed Set of Categories

If you have a small, fixed set of categories, you can also store each category as a boolean field in the metadata to enable fast filtering.

For example: searching for documents related to "roles" or "permissions".

Adding a document with categorical metadata:

```python
vector_collection.add(
    documents=["Document about user roles and permissions."],
    metadatas=[
        {
            "category_roles": True,
            "category_permissions": True,
            "category_users": True,
        }
    ],
    ids=[str(uuid.uuid4())]
)
```

```python
query_results = vector_collection.query(
    query_texts=["What you are searching for"],
    n_results=5,
    where={
        where={"$or": [{"category_roles": True}, {"category_permissions": True}]}
    }
)
```

#### Dynamic Set of Categories

If you set of categories is not-closed, you can search for the category directly, in addition to the search by the query text and supply both sets of results to the generative model as context.

```python
results_by_query = vector_collection.query(
    query_texts=["What you are searching for"],
    n_results=5,
)

results_by_category = vector_collection.query(
    query_texts=["permissions, roles"],
    n_results=5,
)

results_for_context = results_by_query + results_by_category
```

## Metadata Design Patterns

### Filtering-Optimized Metadata

Structure metadata to enable efficient filtering.

**Best Practices**:

- Use consistent data types
- Index frequently filtered fields
- Avoid deeply nested structures
- Use arrays for multi-value attributes

```json
{
  "metadata": {
    "category": "product", // String - exact match
    "price_range": "100-200", // String - range queries  
    "features": ["wireless", "bt5"], // Array - contains queries
    "in_stock": true, // Boolean - exact match
    "created_date": "2025-01-15", // Date - range queries
    "rating": 4.5 // Number - range queries
  }
}
```

### Temporal Metadata

Include time-based attributes for time-aware retrieval.

```json
{
  "metadata": {
    "created_at": "2025-01-15T10:30:00Z",
    "updated_at": "2025-01-20T14:22:00Z", 
    "published_date": "2025-01-16",
    "expiry_date": "2025-12-31",
    "season": "winter",
    "year": 2025,
    "quarter": "Q1"
  }
}
```

### Access Control Metadata

Include security and permissions information.

```json
{
  "metadata": {
    "visibility": "public",
    "access_level": "internal", 
    "department": "engineering",
    "classification": "confidential",
    "allowed_roles": ["developer", "manager"],
    "owner": "john.doe@company.com"
  }
}
```

## Collection Design Strategies

### Single Collection Approach

Store all vectors in one collection with metadata differentiation.

**Advantages**:

- Simpler architecture
- Cross-type similarity search

**Disadvantages**:

- Potential performance impact
- Complex filtering logic

### Multi-Collection Approach

Separate collections for different data types or use cases.

**Example Collections**:

- `documents` - Text documents and articles
- `products` - E-commerce product data  
- `users` - User profiles and preferences
- `images` - Image embeddings and metadata

**Advantages**:

- Optimized for specific use cases
- Better performance isolation
- Easier to manage and scale

**Disadvantages**:

- More complex architecture
- No cross-collection search

## Performance Optimization

### Vector Dimensionality

Choose embedding dimensions based on your use case:

- **384 dimensions**: Good balance for most text applications
- **768 dimensions**: Higher quality for complex documents  
- **1536 dimensions**: Maximum quality for critical applications
- **128-256 dimensions**: Faster search for simple use cases

### Indexing Strategies

- **HNSW (Hierarchical Navigable Small Worlds)**: Best general-purpose index
- **IVF (Inverted File)**: Good for large datasets with memory constraints
- **Flat**: Simple but only for smaller datasets

### Batch Operations

Insert documents in batches for better performance:

```python
# Good - batch processing
documents = [doc1, doc2, doc3, ...]
embeddings = embedding_model.encode(documents)
collection.add(embeddings=embeddings, metadatas=metadatas, ids=ids)

# Avoid - single document processing  
for doc in documents:
    embedding = embedding_model.encode([doc])
    collection.add(embeddings=[embedding], metadatas=[metadata], ids=[id])
```

## Best Practices

### Data Preparation

1. **Clean text data**: Remove noise, normalize formats
2. **Consistent preprocessing**: Apply same preprocessing to queries and documents
3. **Chunk strategically**: Balance context preservation with search precision
4. **Version your embeddings**: Track embedding model versions

### Metadata Management

1. **Index frequently filtered fields**: Optimize for common query patterns
2. **Use consistent schemas**: Standardize metadata structure across documents
3. **Include relevance signals**: Add fields that help ranking (scores, popularity)
4. **Plan for updates**: Design for incremental metadata updates

### Search Strategy

1. **Hybrid search**: Combine vector similarity with metadata filtering
2. **Re-ranking**: Use additional signals to improve initial vector search results
3. **Query expansion**: Use multiple query variations for better recall
4. **Feedback loops**: Track and learn from user interactions

---

**Navigation:**

- Previous page: [Data modelling and distributed file storage](./data-modelling-distributed-file-storage.md)
- Next page: [Data modelling summary](./data-modelling-summary.md)
